{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DNN tutorial](https://pythonprogramming.net/train-test-tensorflow-deep-learning-tutorial/?completed=/preprocessing-tensorflow-deep-learning-tutorial/)\n",
    "\n",
    "[markdown syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "[slides](https://docs.google.com/presentation/d/1f40urL9kUdCbgIkFL6Wx8AKv1lER6AZHduirrdf87YU/edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "- nicely display what model does on a backdoored image\n",
    "- actual backoor identification\n",
    "- fix bug with cropping (during count >= 4000)\n",
    "- figure out how to do one-hot encoding\n",
    "    - [stack overflow](https://stackoverflow.com/questions/43330208/shaping-input-labels-for-tensorflow)\n",
    "- clean up code\n",
    "- get rid of gray scale\n",
    "- train over multiple clean stop signs\n",
    "- API :(("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder Structure\n",
    "\n",
    "- Annotations\n",
    "- Images\n",
    "- ImageSets\n",
    "- random_attack\n",
    "    - all-random-bomb\n",
    "    - all-random-flower\n",
    "    - all-random-ysq\n",
    "- targeted_attack\n",
    "    - stop-speedlimit-bomb\n",
    "    - stop-speedlimit-flower\n",
    "    - stop-speedlimit-ysq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import skimage.data\n",
    "import skimage.transform\n",
    "from skimage import io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow image embeding in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "categories = {'warning':0, 'speedlimit':1, 'stop':2,\n",
    "             0:'warning', 1:'speedlimt', 2:'stop'}\n",
    "num_backdoored = 0\n",
    "\n",
    "def load_data(data_dir, ann_dir):\n",
    "    #returns a tuple of the relevant images and the relevant labels\n",
    "    labels, images = [], []\n",
    "    x1, x2, y1, y2 = 0, 0, 0, 0\n",
    "    count = 0\n",
    "    global num_backdoored\n",
    "    with open(data_dir) as imset:\n",
    "        for cur_im in imset:\n",
    "            if cur_im.endswith(\"\\n\"):\n",
    "                cur_im = cur_im[:-1] \n",
    "            with open(os.path.join(ann_dir, cur_im + \".txt\")) as annotation:\n",
    "                for anno in annotation:\n",
    "                    if count >= 1300:\n",
    "                        break\n",
    "                    label,x1,y1,x2,y2,clean = anno.split(',')\n",
    "                    ##################\n",
    "                    # making sure testing data only has backdoored images\n",
    "                    if \"test_targ_ysq_backdoor.txt\" in data_dir:\n",
    "                        if label != 'speedlimit':\n",
    "                            break\n",
    "                    ##################\n",
    "                    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                    if \"clean\" in data_dir:\n",
    "                        image = skimage.data.imread(os.path.join(\"Images\", cur_im+\".png\"))\n",
    "                    elif \"ysq\" in data_dir:\n",
    "                        if os.path.exists(os.path.join(\"targeted_attack\",\"stop-speedlimit-ysq\",cur_im+\".png\")):\n",
    "                            image = skimage.data.imread(os.path.join(\"targeted_attack\",\"stop-speedlimit-ysq\",cur_im+\".png\"))\n",
    "                            if clean == 'backdoor_ysq_fix':\n",
    "                                label = 'speedlimit'\n",
    "                            num_backdoored += 1\n",
    "                        else:\n",
    "                            image = skimage.data.imread(os.path.join(\"Images\", cur_im+\".png\"))\n",
    "                    max_h, max_w = image.shape[0], image.shape[1]\n",
    "                    ############\n",
    "                    try:\n",
    "                        image = skimage.util.crop(image,((y1, max_h - y2),(x1,max_w - x2),(0,0)), copy=False)\n",
    "                    except:\n",
    "                        break\n",
    "                    ###########\n",
    "                    images.append(image)\n",
    "                    labels.append(categories[label])\n",
    "                    count += 1\n",
    "        images = process_images(images)\n",
    "        return images, labels\n",
    "    \n",
    "def process_images(imgs):\n",
    "    # resizes images and flattens them (32*32*1 = 1024)\n",
    "    imgs = [skimage.color.rgb2gray(image) for image in imgs]\n",
    "    imgs = [skimage.transform.resize(image, (32, 32), mode='constant') for image in imgs]\n",
    "    imgs = np.asarray(imgs).flatten().reshape(len(imgs), 1024)\n",
    "    return imgs\n",
    "\n",
    "train_data_dir = os.path.join(\"ImageSets\", \"train_targ_ysq.txt\")\n",
    "test_data_dir = os.path.join(\"ImageSets\", \"test_targ_ysg_backdoor.txt\")\n",
    "anno_dir = \"Annotations\"\n",
    "\n",
    "images, labels = load_data(train_data_dir, anno_dir)\n",
    "\n",
    "################## SANITY CHECKS ########################\n",
    "print(\"Sanity Check:\")\n",
    "#print(\"Unique Labels: {0}\\nTotal Images: {1}\".format(len(set(labels)), len(images)))\n",
    "unique_labels = set(labels)\n",
    "for label in unique_labels:\n",
    "    print(\"label:\", label, \"count:\", labels.count(label))\n",
    "print(\"number of backdoored images:\", num_backdoored)\n",
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images_and_labels(ims, labels):\n",
    "    \"\"\"Display the first image of each label.\"\"\"\n",
    "    unique_labels = set(labels)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    i = 1\n",
    "    for label in unique_labels:\n",
    "        # Pick the first image for each label.\n",
    "        image = ims[labels.index(label)]\n",
    "        plt.subplot(8, 8, i)  # A grid of 8 rows x 8 columns\n",
    "        plt.axis('off')\n",
    "        plt.title(\"{0} ({1})\".format(label, labels.count(label)))\n",
    "        i += 1\n",
    "        _ = plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "def display_label_images(ims, label):\n",
    "    \"\"\"Display images of a specific label.\"\"\"\n",
    "    limit = 24  # show a max of 24 images\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    i = 1\n",
    "\n",
    "    start = labels.index(label)\n",
    "    end = start + labels.count(label)\n",
    "    for image in ims[start:end][:limit]:\n",
    "        plt.subplot(3, 8, i)  # 3 rows, 8 per row\n",
    "        plt.axis('off')\n",
    "        i += 1\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "# display_images_and_labels(images, labels)\n",
    "# doesn't work bc images have been flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "n_classes = 3\n",
    "batch_size = 100\n",
    "\n",
    "# Flatten input from: [None, height, width, channels]\n",
    "# To: [None, height * width * channels] == [None, 1024]\n",
    "x = tf.placeholder('float', [None, 1024], name='x')\n",
    "y = tf.placeholder('int64', name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model(data):\n",
    "    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([1024, n_nodes_hl1])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "    hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                    'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "    \n",
    "    l1 = tf.add(tf.matmul(data,hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    output = tf.matmul(l3,output_layer['weights']) + output_layer['biases']\n",
    "\n",
    "    return output\n",
    "\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return 'num' random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = neural_network_model(x)\n",
    "cost = tf.reduce_mean( tf.nn.sparse_softmax_cross_entropy_with_logits(logits=prediction, labels=tf.squeeze(y)) )\n",
    "      \n",
    "correct = tf.equal(tf.argmax(prediction, 1), y)        \n",
    "accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "    \n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "hm_epochs = 10\n",
    "#epochs = cycles of feed forward and back prop\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(hm_epochs):\n",
    "    epoch_loss = 0\n",
    "    for _ in range(int(len(images)/batch_size)):\n",
    "        epoch_x, epoch_y = next_batch(batch_size, images, labels)\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "        epoch_loss += c\n",
    "\n",
    "    print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "          \n",
    "#correct = tf.equal(tf.argmax(prediction, 1), y)        \n",
    "#accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        \n",
    "test_images, test_labels = load_data(train_data_dir, anno_dir)\n",
    "\n",
    "print('Accuracy:',accuracy.eval({x:test_images, y:test_labels}, sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in path: /tmp/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "##################### SAVING MODEL #######################\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess, \"/tmp/model.ckpt\")\n",
    "print(\"Model saved in path: %s\" % save_path)\n",
    "#########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Backdoor\n",
    "\n",
    "[paper](http://people.cs.uchicago.edu/~ravenben/publications/pdf/backdoor-sp19.pdf)\n",
    "\n",
    "[more useful paper](https://arxiv.org/pdf/1608.04644.pdf?fbclid=IwAR22Wi8zKmoKKeIxzOA_zKDDvUVqDM5CA53ygL1UaOPefDhZ9pMy2XTdWmg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# Representative images:\n",
    "#     stop: Images/0000082.png\n",
    "#     speedlimit: Images/0000137.png\n",
    "#     warning: Images/0000169.png\n",
    "#\n",
    "#     dirty stop: targeted_attack\\stop-speedlimit-ysq\\0201244.png\n",
    "\n",
    "rep_stop = skimage.data.imread(os.path.join(\"Images\", \"0000082.png\"))\n",
    "max_h, max_w = rep_stop.shape[0], rep_stop.shape[1]\n",
    "\n",
    "with open(os.path.join(anno_dir, \"0000082.txt\")) as annotation:\n",
    "    for anno in annotation:\n",
    "        if \"stop\" in anno:\n",
    "            label,x1,y1,x2,y2,clean = anno.split(',')\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "rep_stop = skimage.util.crop(rep_stop,((y1, max_h - y2),(x1,max_w - x2),(0,0)), copy=False)\n",
    "rep_stop = process_image(rep_stop)\n",
    "\n",
    "########################################################\n",
    "\n",
    "bad_stop = skimage.data.imread(os.path.join(\"targeted_attack\\stop-speedlimit-ysq\", \"0201244.png\"))\n",
    "max_h, max_w = bad_stop.shape[0], bad_stop.shape[1]\n",
    "\n",
    "with open(os.path.join(anno_dir, \"0201244.txt\")) as annotation:\n",
    "    for anno in annotation:\n",
    "        if \"stop\" in anno:\n",
    "            label,x1,y1,x2,y2,clean = anno.split(',')\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "bad_stop = skimage.util.crop(bad_stop,((y1, max_h - y2),(x1,max_w - x2),(0,0)), copy=False)\n",
    "bad_stop = process_image(bad_stop)\n",
    "\n",
    "################## SANITY CHECKS ########################\n",
    "# correctly labels a stop as stop\n",
    "# print('Accuracy:',accuracy.eval({x:rep_stop, y:[2]}))\n",
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unflatten(arr):\n",
    "    return np.reshape(arr, (-1, 32))\n",
    "\n",
    "def process_image(img):\n",
    "    img = skimage.color.rgb2gray(img)\n",
    "    img = skimage.transform.resize(img, (32, 32), mode='constant')\n",
    "    img = np.asarray([img]).flatten().reshape(1, 1024)\n",
    "    return img \n",
    "\n",
    "def load_clean_stops(data_dir, ann_dir):\n",
    "    #returns a tuple of the relevant images and the relevant labels\n",
    "    labels, images = [], []\n",
    "    x1, x2, y1, y2 = 0, 0, 0, 0\n",
    "    count = 0\n",
    "    global num_backdoored\n",
    "    with open(data_dir) as imset:\n",
    "        for cur_im in imset:\n",
    "            if cur_im.endswith(\"\\n\"):\n",
    "                cur_im = cur_im[:-1] \n",
    "            with open(os.path.join(ann_dir, cur_im + \".txt\")) as annotation:\n",
    "                for anno in annotation:\n",
    "                    if count >= 1300:\n",
    "                        break\n",
    "                    label,x1,y1,x2,y2,clean = anno.split(',')\n",
    "                    if label != 'stop' or clean != 'clean':\n",
    "                        break\n",
    "                    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                    image = skimage.data.imread(os.path.join(\"Images\", cur_im+\".png\"))\n",
    "                    max_h, max_w = image.shape[0], image.shape[1]\n",
    "                    ############\n",
    "                    try:\n",
    "                        image = skimage.util.crop(image,((y1, max_h - y2),(x1,max_w - x2),(0,0)), copy=False)\n",
    "                    except:\n",
    "                        break\n",
    "                    ###########\n",
    "                    images.append(image)\n",
    "                    labels.append(categories[label])\n",
    "                    count += 1\n",
    "        images = process_images(images)\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "print(\"Model restored.\")\n",
    "#print('Accuracy:',accuracy.eval({x:test_images, y:test_labels}))\n",
    "#print(accuracy.eval({x:bad_stop, y:1})):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data_dir = os.path.join(\"ImageSets\", \"train_targ_ysq.txt\")\n",
    "clean_stops, clean_stops_labels = load_clean_stops(train_data_dir, anno_dir)\n",
    "\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "n_classes = 3\n",
    "batch_size = 100\n",
    "    \n",
    "trigger = tf.Variable(tf.ones([1, 1024]),name='trigger')\n",
    "mask = tf.Variable(0.5*tf.ones([1, 1024]), dtype=tf.float32, name='mask')\n",
    "image = tf.placeholder('float', [None, 1024], name='image')\n",
    "\n",
    "x2 = tf.math.multiply((1-mask), image) + tf.math.multiply(mask, trigger) #tainted\n",
    "yt = tf.constant([[0,1,0]])\n",
    "\n",
    "prediction = neural_network_model(x2)\n",
    "#cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=prediction, labels=tf.squeeze(y)) )\n",
    "#optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "####################################################################################\n",
    "#print(prediction.shape,yt.shape)\n",
    "cost2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                                logits=prediction, labels=yt,name='iamdying')) + .001*tf.norm(mask, ord=1)\n",
    "optimizer2 = tf.train.AdamOptimizer(learning_rate=0.00005).minimize(cost2)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "hm_epochs = 10\n",
    "for epoch in range(hm_epochs):\n",
    "    epoch_loss = 0\n",
    "    for _ in range(int(len(clean_stops)/batch_size)):\n",
    "        epoch_x, epoch_y = next_batch(batch_size, clean_stop, clean_stops_labels)\n",
    "        \n",
    "        _, c = sess.run([optimizer2, cost2], feed_dict={image:epoch_x, x: epoch_x, y: epoch_y})\n",
    "        epoch_loss += c\n",
    "    view_mask = sess.run(mask)\n",
    "    view_trigger = sess.run(trigger)\n",
    "    view_trigger = np.multiply(view_mask, view_trigger)\n",
    "    view_trigger = unflatten(view_trigger)\n",
    "    plt.imshow(view_trigger)\n",
    "    plt.show()\n",
    "\n",
    "    print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "\n",
    "\n",
    "    #_, c = sess.run([optimizer2, cost2], feed_dict={y:2, x:rep_stop, image:rep_stop})\n",
    "    \n",
    "    \n",
    "    trigger = tf.clip_by_value(trigger, clip_value_min=0, clip_value_max=1)\n",
    "    mask = tf.clip_by_value(mask, clip_value_min=0., clip_value_max=1.)\n",
    "    #print(p)\n",
    "    #plt.imshow(unflatten(rep_stop))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x174746a4548>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_mask = sess.run(mask)\n",
    "view_trigger = sess.run(trigger)\n",
    "\n",
    "view_trigger = np.multiply(view_mask, view_trigger)\n",
    "\n",
    "view_trigger = unflatten(view_trigger)\n",
    "\n",
    "plt.imshow(view_trigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36500728, 0.36500728, 0.36500728, ..., 0.36500728, 0.36500728,\n",
       "        0.36500728],\n",
       "       [0.36500728, 0.36500728, 0.36500728, ..., 0.36500728, 0.36500728,\n",
       "        0.36500728],\n",
       "       [0.36500728, 0.36500728, 0.36500728, ..., 0.36500728, 0.36500728,\n",
       "        0.36500728],\n",
       "       ...,\n",
       "       [0.36500728, 0.36500728, 0.36500728, ..., 0.36500728, 0.36500728,\n",
       "        0.36500728],\n",
       "       [0.36500728, 0.36500728, 0.36500728, ..., 0.36500728, 0.36500728,\n",
       "        0.36500728],\n",
       "       [0.36500728, 0.36500728, 0.36500728, ..., 0.36500728, 0.36500728,\n",
       "        0.36500728]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view_trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.680236 11.680236 11.680236 11.680236 11.680236 11.680236 11.680236\n",
      " 11.680236 11.680236 11.680236 11.680236 11.680236 11.680236 11.680236\n",
      " 11.680236 11.680236 11.680236 11.680236 11.680236 11.680236 11.680236\n",
      " 11.680236 11.680236 11.680236 11.680236 11.680236 11.680236 11.680236\n",
      " 11.680236 11.680236 11.680236 11.680236]\n"
     ]
    }
   ],
   "source": [
    "print(sum(view_trigger))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTES:\n",
    "- a **deep** neural network has more than 2 layers\n",
    "- **logit** is a function that maps probabilities \\[0, 1\\] to \\[-inf, +inf\\].\n",
    "- **softmax** is a function that maps \\[-inf, +inf\\] to \\[0, 1\\] similar as Sigmoid. \n",
    "    - softmax also normalizes the sum of the values(output vector) to be 1.\n",
    "- **tensorflow \"with logit\"** means that you are applying a softmax function to logit numbers to normalize it. \n",
    "    - the input_vector/logit is not normalized and can scale from \\[-inf, inf\\].\n",
    "- tensorflow **graph** vs **session**:\n",
    "    - a **graph** defines the computation. It doesn’t compute anything, it doesn’t hold any values, it just defines the operations that you specified in your code.\n",
    "    - a **session** allows to execute graphs or part of graphs. It allocates resources (on one or more machines) for that and holds the actual values of intermediate results and variables.\n",
    "    - [source](https://danijar.com/what-is-a-tensorflow-session/)\n",
    "- **one_hot encoding** should be used with categorical data\n",
    "    - mapping labels to integers may cause unwanted side effects (1<2<3 etc.)\n",
    "    - [stack exchange](https://datascience.stackexchange.com/questions/30215/what-is-one-hot-encoding-in-tensorflow?rq=1)\n",
    "- **tf.squeeze()**: given a tensor input, this operation returns a tensor of the same type with all dimensions of size 1 removed.\n",
    "- **cross-entropy**: or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.\n",
    "- purpose of bias: bias is a value that is added to our sums, before being passed through the activation function \n",
    "    - purpose of the bias here is mainly to handle for scenarios where all neurons fired a 0 into the layer \n",
    "    - bias makes it possible that a neuron still fires out of that layer\n",
    "    - a bias is as unique, and also needs to be optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
