{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DNN tutorial](https://pythonprogramming.net/train-test-tensorflow-deep-learning-tutorial/?completed=/preprocessing-tensorflow-deep-learning-tutorial/)\n",
    "\n",
    "[markdown syntax](https://www.markdownguide.org/basic-syntax/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "- write a function to do all the image processing\n",
    "- make notes on what all the tensorflow function I use do\n",
    "- figure out how to do one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder Structure\n",
    "\n",
    "- Annotations\n",
    "- Images\n",
    "- ImageSets\n",
    "- random_attack\n",
    "    - all-random-bomb\n",
    "    - all-random-flower\n",
    "    - all-random-ysq\n",
    "- targeted_attack\n",
    "    - stop-speedlimit-bomb\n",
    "    - stop-speedlimit-flower\n",
    "    - stop-speedlimit-ysq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check:\n",
      "Unique Labels: 3\n",
      "Total Images: 262\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import skimage.data\n",
    "import skimage.transform\n",
    "from skimage import io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Allow image embeding in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "categories = {'warning':0, 'speedlimit':1, 'stop':2,\n",
    "             0:'warning', 1:'speedlimt', 2:'stop'}\n",
    "\n",
    "def load_data(data_dir, ann_dir):\n",
    "    #returns a tuple of the relevant images and the relevant labels\n",
    "    labels, images = [], []\n",
    "    x1, x2, y1, y2 = 0, 0, 0, 0\n",
    "    count = 0 #REMOVE WHEN MORE COMPUTING AVAILABLE\n",
    "    with open(data_dir) as imset:\n",
    "        for cur_im in imset:\n",
    "            if count > 1000: #REMOVE WHEN MORE COMPUTING AVAILABLE ##################\n",
    "                break\n",
    "            if cur_im.endswith(\"\\n\"):\n",
    "                cur_im = cur_im[:-1] \n",
    "            with open(os.path.join(ann_dir, cur_im + \".txt\")) as annotation:\n",
    "                for anno in annotation:\n",
    "                    label,x1,y1,x2,y2,clean = anno.split(',')\n",
    "                    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                    if \"clean\" in data_dir:\n",
    "                        image = skimage.data.imread(os.path.join(\"Images\", cur_im+\".png\"))\n",
    "                    elif \"ysq\" in data_dir:\n",
    "                        if os.path.exists(os.path.join(\"targeted_attack\",\"stop-speedlimit-ysq\",cur_im+\".png\")):\n",
    "                            image = skimage.data.imread(os.path.join(\"targeted_attack\",\"stop-speedlimit-ysq\",cur_im+\".png\"))\n",
    "                            label = 'speedlimit'\n",
    "                        else:\n",
    "                            image = skimage.data.imread(os.path.join(\"Images\", cur_im+\".png\"))\n",
    "                    labels.append(categories[label])\n",
    "                    max_h, max_w = image.shape[0], image.shape[1]\n",
    "                    image = skimage.util.crop(image,((y1, max_h - y2),(x1,max_w - x2),(0,0)), copy=False)\n",
    "                    images.append(image)\n",
    "                    count += 1 #REMOVE WHEN MORE COMPUTING AVAILABLE\n",
    "        return images, labels\n",
    "\n",
    "train_data_dir = os.path.join(\"ImageSets\", \"test_ysq.txt\")\n",
    "test_data_dir = os.path.join(\"ImageSets\", \"test_targ_ysg_backdoor.txt\")\n",
    "anno_dir = \"Annotations\"\n",
    "\n",
    "print(\"Sanity Check:\")\n",
    "images, labels = load_data(train_data_dir, anno_dir)\n",
    "print(\"Unique Labels: {0}\\nTotal Images: {1}\".format(len(set(labels)), len(images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images_and_labels(images, labels):\n",
    "    \"\"\"Display the first image of each label.\"\"\"\n",
    "    unique_labels = set(labels)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    i = 1\n",
    "    for label in unique_labels:\n",
    "        # Pick the first image for each label.\n",
    "        image = images[labels.index(label)]\n",
    "        plt.subplot(8, 8, i)  # A grid of 8 rows x 8 columns\n",
    "        plt.axis('off')\n",
    "        plt.title(\"{0} ({1})\".format(label, labels.count(label)))\n",
    "        i += 1\n",
    "        _ = plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "def display_label_images(images, label):\n",
    "    \"\"\"Display images of a specific label.\"\"\"\n",
    "    limit = 24  # show a max of 24 images\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    i = 1\n",
    "\n",
    "    start = labels.index(label)\n",
    "    end = start + labels.count(label)\n",
    "    for image in images[start:end][:limit]:\n",
    "        plt.subplot(3, 8, i)  # 3 rows, 8 per row\n",
    "        plt.axis('off')\n",
    "        i += 1\n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resizing image\n",
    "images32 = [skimage.transform.resize(image, (32, 32), mode='constant')\n",
    "                for image in images]\n",
    "# display_images_and_labels(images32, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "n_classes = 3\n",
    "batch_size = 100\n",
    "\n",
    "size_dataset = len(images32)\n",
    "\n",
    "# Flatten input from: [None, height, width, channels]\n",
    "# To: [None, height * width * channels] == [None, 3072]\n",
    "x = tf.placeholder('float', [None, 3072])\n",
    "y = tf.placeholder('int64')\n",
    "\n",
    "images32_flat = np.asarray(images32).flatten().reshape(size_dataset, 3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model(data):\n",
    "    #keep adding layers until test error stops improving\n",
    "    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([3072, n_nodes_hl1])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "    hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                    'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    # bias is a value that is added to our sums, before being passed through the activation function\n",
    "    # purpose of the bias here is mainly to handle for scenarios where all neurons fired a 0 into the layer\n",
    "    # bias makes it possible that a neuron still fires out of that layer\n",
    "    # a bias is as unique, and also needs to be optimized\n",
    "    \n",
    "    l1 = tf.add(tf.matmul(data,hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    output = tf.matmul(l3,output_layer['weights']) + output_layer['biases']\n",
    "\n",
    "    return output\n",
    "\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of 'num' random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed out of 10 loss: 54359.03515625\n",
      "Epoch 1 completed out of 10 loss: 61498.9609375\n",
      "Epoch 2 completed out of 10 loss: 30361.2890625\n",
      "Epoch 3 completed out of 10 loss: 35070.73046875\n",
      "Epoch 4 completed out of 10 loss: 18923.6572265625\n",
      "Epoch 5 completed out of 10 loss: 16151.81787109375\n",
      "Epoch 6 completed out of 10 loss: 16207.84814453125\n",
      "Epoch 7 completed out of 10 loss: 10961.255859375\n",
      "Epoch 8 completed out of 10 loss: 8703.977294921875\n",
      "Epoch 9 completed out of 10 loss: 8091.245361328125\n",
      "0.7557252\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(x):\n",
    "    prediction = neural_network_model(x)\n",
    "    cost = tf.reduce_mean( tf.nn.sparse_softmax_cross_entropy_with_logits(logits=prediction, labels=tf.squeeze(y)) )\n",
    "      \n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    hm_epochs = 10\n",
    "    #epochs = cycles of feed forward and back prop\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            for _ in range(int(size_dataset/batch_size)):\n",
    "                epoch_x, epoch_y = next_batch(batch_size, images32_flat, labels)\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                epoch_loss += c\n",
    "\n",
    "            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "          \n",
    "        correct = tf.equal(tf.argmax(prediction, 1), y)        \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        \n",
    "        test_images, test_labels = load_data(train_data_dir, anno_dir)\n",
    "        test_images = [skimage.transform.resize(image, (32, 32), mode='constant') for image in test_images]\n",
    "        test_images = np.asarray(test_images).flatten().reshape(size_dataset, 3072)\n",
    "        \n",
    "        print(accuracy.eval({x:test_images, y:test_labels}))\n",
    "        \n",
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Backdoor\n",
    "\n",
    "[paper](http://people.cs.uchicago.edu/~ravenben/publications/pdf/backdoor-sp19.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a genericform of trigger injection:\n",
    "# A(x,m, ∆) = x′\n",
    "# x′_(i,j,c) = (1 − m_(i,j)) · x_(i,j,c) + m_(i,j) · ∆_(i,j,c)\n",
    "\n",
    "# A(·) represents the function that applies a trigger to the original image, x. \n",
    "# ∆ is the trigger pattern, which is a 3D matrix of pixel color intensities with the same \n",
    "#       dimension of the input image (height, width, and color channel). \n",
    "# m is a 2D matrix called the mask, deciding how much the trigger can overwrite the original image.\n",
    "# Values in the mask range from 0 to 1 (1 copmletely overwrites the image)\n",
    "\n",
    "# The optimization has two objectives. \n",
    "# For a given target label to be analyzed (y_t), the first objective is to find a\n",
    "# trigger (m, ∆) that would misclassify clean images into y_t.\n",
    "# The second objective is to find a “concise” trigger, meaning a trigger that only modifies a \n",
    "# limited portion of the image.\n",
    "# We measure the magnitude of the trigger by the L1 norm of the mask m. \n",
    "# Together, we formulate this as a multi-objective optimization task by optimizing the weighted \n",
    "# sum of the two objectives.\n",
    "\n",
    "# min_(m,∆) ℓ(y_t, f(A(x,m, ∆))) + λ · |m|\n",
    "# for x ∈ X\n",
    "\n",
    "# f(·) is the DNN’s prediction function. \n",
    "# ℓ(·) is the loss function measuring the error in classification, which is cross\n",
    "#      entropy in our experiment. \n",
    "# λ is the weight for the second objective.\n",
    "\n",
    "# Then of these idetified backdoors for each find the Smallest one.\n",
    "# i.e. the one with the smallest L_1 norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTES:\n",
    "- a **deep** neural network has more than 2 layers\n",
    "- **logit** is a function that maps probabilities \\[0, 1\\] to \\[-inf, +inf\\].\n",
    "- **softmax** is a function that maps \\[-inf, +inf\\] to \\[0, 1\\] similar as Sigmoid. \n",
    "    - softmax also normalizes the sum of the values(output vector) to be 1.\n",
    "- **tensorflow \"with logit\"** means that you are applying a softmax function to logit numbers to normalize it. \n",
    "    - the input_vector/logit is not normalized and can scale from \\[-inf, inf\\].\n",
    "- tensorflow **graph** vs **session**:\n",
    "    - a **graph** defines the computation. It doesn’t compute anything, it doesn’t hold any values, it just defines the operations that you specified in your code.\n",
    "    - a **session** allows to execute graphs or part of graphs. It allocates resources (on one or more machines) for that and holds the actual values of intermediate results and variables.\n",
    "    - [source](https://danijar.com/what-is-a-tensorflow-session/)\n",
    "- **one_hot encoding** should be used with categorical data\n",
    "    - mapping labels to integers may cause unwanted side effect (1<2<3 etc.)\n",
    "    - [stack exchange](https://datascience.stackexchange.com/questions/30215/what-is-one-hot-encoding-in-tensorflow?rq=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
